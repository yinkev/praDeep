# praDeep Architecture Overview

*Generated by Codex GPT-5.2 with xhigh reasoning - January 2026*

---

## What This Repo Is

`praDeep` is a full-stack "AI tutor" system: a FastAPI backend + a Next.js web UI, built around multi-agent workflows (solve, research, question generation, guided learning, co-writer, ideagen, chat) and a local knowledge-base store for RAG.

---

## High-Level Architecture

```
Browser (Next.js, `web/`) ── REST + WebSocket ──> FastAPI (`src/api/`)
                                            │
                                            v
                                  Agent Workflows (`src/agents/*`)
                                            │
                 ┌──────────────────────────┼───────────────────────────┐
                 v                          v                           v
         Tools (`src/tools/*`)     Shared Services (`src/services/*`)   KB/Storage (`src/knowledge/*`)
                 │                          │                           │
                 └──────────────────────────┴───────────────┬───────────┘
                                                             v
                                                      Filesystem `data/`
```

- The backend is a single Python process that orchestrates agent pipelines and streams progress/logs over WebSockets for "long" operations.
- Persistence is filesystem-first: outputs, notebooks, provider configs, and knowledge bases live under `data/` and are served back to the UI via a static mount.

---

## Repository Layout (Top-Level)

| Directory | Purpose |
|-----------|---------|
| `src/` | Python backend + core logic (agents, services, tools, KB, logging) |
| `web/` | Next.js 16 + React 19 frontend (App Router) |
| `config/` | YAML configuration (`main.yaml`, `agents.yaml`) |
| `data/` | Runtime storage (knowledge bases + user outputs) |
| `scripts/` | Launch/install helpers (`start_web.py`, `start.py`) |
| `docs/` | VitePress documentation site |
| `tests/` | Test suite (prompt manager + RAG retrievers + reranker + embedding adapter mapping) |

---

## Backend API Layer (FastAPI)

### Entry Point
- Creates the app, mounts static outputs, and wires routers (`src/api/main.py`)
- Static serving: mounts `data/user` at `/api/outputs` so the UI can fetch artifacts like images, markdown, audio, etc.

### Routers (Feature-Scoped)

| Router | Endpoint | Description |
|--------|----------|-------------|
| **Solve** | WebSocket | Streaming solve runs |
| **Question** | WebSocket | Custom generation and mimic-exam workflows |
| **Research** | WebSocket | Deep-research pipeline + topic optimization |
| **Knowledge** | REST + WS | KB CRUD/upload/refresh + progress WebSockets |
| **Guide** | REST + WS | Session-based learning |
| **Co-writer** | REST + WS | Edit actions + narration |
| **Notebook** | REST | Notebook management |
| **IdeaGen** | REST + WS | Idea generation |
| **Chat** | REST + WS | Multi-turn chat |
| **Settings** | REST | System settings |
| **LLM Provider** | REST | LLM provider management |
| **Embedding Provider** | REST | Embedding provider management |

### Streaming Design Pattern
- For long tasks, the API accepts a WS connection, assigns a `task_id`, attaches a `LogInterceptor` to the relevant logger
- Pushes structured events: `log`, `status`, `agent_status`, `token_stats`, `progress`, `result`
- KB ingestion progress uses a `ProgressTracker` that saves `.progress.json` and broadcasts via `ProgressBroadcaster`

---

## Agent/Workflow Layer

### Shared Base
- Most agents inherit a unified `BaseAgent` that centralizes:
  - Prompt loading
  - LLM calls
  - Temperature/max_tokens sourcing
  - Logging
  - Token stats

### Prompt Loading
- YAML prompts per module/agent with language fallback + caching (`src/services/prompt/manager.py`)

---

### Solve Agent (Dual-Loop)

**Orchestrator:** `MainSolver` runs **Analysis Loop → Solve Loop → Response/Formatting**, persisting task state into a timestamped folder.

#### Analysis Loop
| Agent | Function |
|-------|----------|
| `InvestigateAgent` | Picks tool actions (RAG, numbered-item lookup, optional web search) and writes "knowledge items" into `InvestigateMemory` |
| `NoteAgent` | Compresses raw tool output into usable summaries and updates citation memory |

#### Solve Loop
| Agent | Function |
|-------|----------|
| `ManagerAgent` | Generates step-level plan from the question + gathered knowledge |
| `SolveAgent` | Decides which tools to call per step and records a tool-call plan into `SolveMemory` |
| `ToolAgent` | Executes tool calls (rag naive/hybrid, web search, code execution), summarizes results, writes artifacts |
| `ResponseAgent` | Generates the step response using tool summaries + artifact references and citations |
| `PrecisionAnswerAgent` | Optionally produces a concise "precision answer" |

#### Citations
- Tracked as persistent JSON (`src/agents/solve/memory/citation_memory.py`)
- Incorporated into markdown output

---

### Research Agent (DR-in-KG 2.0)

**Orchestrator:** `ResearchPipeline` implements **Planning → Researching (dynamic queue) → Reporting**

- Uses `DynamicTopicQueue` and centralized citation manager
- Tool selection is explicit and configurable:
  - `rag_search`
  - `web_search`
  - `PaperSearchTool` (arXiv)
  - `query_numbered_item`
  - `run_code`
- Each call wrapped in timeout+retry helpers

---

### Question Generation Agent

**Orchestrator:** `AgentCoordinator` coordinates a ReAct-style `QuestionGenerationAgent` and `QuestionValidationWorkflow`

#### Two Modes
| Mode | Endpoint | Description |
|------|----------|-------------|
| **Custom** | `WS /api/v1/question/generate` | Generates N questions from structured requirement payload, validates relevance |
| **Mimic** | `WS /api/v1/question/mimic` | Parses exam PDF (MinerU via RAG-Anything), extracts reference questions, generates similar ones |

---

### Other Agents

| Agent | Description |
|-------|-------------|
| **Guide** | Session-based learning plan + HTML generation + contextual chat; state saved per session |
| **Co-writer** | Edit actions (rewrite/shorten/expand + optional RAG/web) and narration (LLM script + optional TTS) |
| **IdeaGen** | Extracts knowledge points from notebook records, runs multi-stage idea filtering/generation |
| **Chat** | Multi-turn chat with optional RAG/web augmentation and persisted sessions |

---

## Shared Services (Cross-Cutting Infrastructure)

### Configuration
| File | Purpose |
|------|---------|
| `config/main.yaml` | Non-secret system/module/tool settings |
| `config/agents.yaml` | Single source of truth for agent `temperature` and `max_tokens` per module |
| Settings API | Can update runtime env vars and persist to `.env` |

### LLM Stack
- `LLMConfig` comes from either `.env` or active provider in `data/user/llm_providers.json`
- Central router `complete()/stream()` picks cloud vs local behavior based on `LLM_MODE` and base URL
- Delegates to `cloud_provider.py` and `local_provider.py`

### Embeddings Stack
- Similar provider system using `data/user/embedding_providers.json` or `.env`
- `EmbeddingClient` delegates to provider adapters:
  - OpenAI-compatible
  - Cohere
  - Jina
  - Ollama
  - Qwen3-VL
  - etc.

### RAG Stack
- `RAGService` is the single entrypoint
- Selects pipeline by `RAG_PROVIDER` (default: `raganything`)

#### Pipelines
| Pipeline | Description |
|----------|-------------|
| `raganything` | Wraps external `raganything` library (MinerU parsing + LightRAG KG) |
| `lightrag` | Internal composable pipeline |
| `academic` | Academic-focused pipeline |
| `llamaindex` | LlamaIndex-based pipeline |

- Hybrid retrieval optionally reranks dense context using `RerankerService`

### Logging & Streaming
- Unified `Logger` writes console + file logs
- WebSocket handler turns log records into structured WS messages
- LightRAG logs can be forwarded into unified logger via `LightRAGLogContext`

---

## Data & Persistence Model

### Knowledge Bases
Location: `data/knowledge_bases/<kb_name>/`

| Directory/File | Purpose |
|----------------|---------|
| `raw/` | Uploaded files |
| `content_list/` | Parsed output |
| `images/` | Extracted images |
| `rag_storage/` | LightRAG storage |
| `numbered_items.json` | Query-item lookups |

### User Outputs
Location: `data/user/`

- Timestamped module runs (solve/question/research)
- Notebooks
- Logs
- Chat sessions
- Code-exec workspace
- `user_history.json`

### API Exposure
- `data/user/**` is reachable under `/api/outputs/**`
- Some routers rewrite markdown links so images resolve correctly from the UI

---

## Frontend (Next.js)

- Next.js App Router project structured by feature routes
- `web/lib/api.ts` constructs REST and WebSocket URLs from `NEXT_PUBLIC_API_BASE`
- `scripts/start_web.py` writes `web/.env.local` so dev server points at backend automatically
- UI is heavily WebSocket-driven for streaming agent runs
- Uses REST for CRUD (KBs, notebooks, settings/providers)

---

## Deployment & Runtime

### Dev Workflow
```bash
# Start backend + frontend
python scripts/start_web.py

# CLI wrapper for major modules
python scripts/start.py
```

### Docker (Optional)
- Multi-stage build creates single container running both services (supervisor)
- Volumes mount `./config` (read-only) and `./data` (read-write)

---

## Notable Mismatches / "Docs vs Code"

| Issue | Details |
|-------|---------|
| **Docs architecture** | `docs/architecture/*` describes databases/auth/Next.js versions that don't match current implementation |
| **Port defaults** | Inconsistent across files; effective behavior uses `BACKEND_PORT`/`FRONTEND_PORT` from `.env` if set |
| **Licensing** | `LICENSE` is AGPL-3.0, but `pyproject.toml` declares Apache-2.0 |
