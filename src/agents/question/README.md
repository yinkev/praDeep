# Question Generation System

Intelligent question generation system with **modular agent architecture** supporting both knowledge base-driven custom generation and reference exam paper mimicking.

## ğŸ“ Directory Structure

```
question/
â”œâ”€â”€ __init__.py                    # Module exports
â”œâ”€â”€ agents/                        # Specialized agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retrieve_agent.py          # Knowledge retrieval from KB
â”‚   â”œâ”€â”€ generate_agent.py          # Question generation
â”‚   â””â”€â”€ relevance_analyzer.py      # Question-KB relevance analysis
â”œâ”€â”€ prompts/                       # Bilingual prompts (YAML)
â”‚   â”œâ”€â”€ zh/                        # Chinese prompts
â”‚   â”‚   â”œâ”€â”€ retrieve_agent.yaml
â”‚   â”‚   â”œâ”€â”€ generate_agent.yaml
â”‚   â”‚   â”œâ”€â”€ relevance_analyzer.yaml
â”‚   â”‚   â””â”€â”€ coordinator.yaml
â”‚   â””â”€â”€ en/                        # English prompts
â”‚       â””â”€â”€ (same structure)
â”œâ”€â”€ coordinator.py                 # Workflow orchestration
â”œâ”€â”€ example.py                     # Usage examples
â””â”€â”€ README.md

# Tools moved to src/tools/question/
src/tools/question/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ pdf_parser.py                  # PDF parsing with MinerU
â”œâ”€â”€ question_extractor.py          # Extract questions from exams
â””â”€â”€ exam_mimic.py                  # Reference-based question generation
```

## ğŸš€ Core Features

### Custom Mode (Knowledge Base-Driven)
- âœ… **Intelligent Search Query Generation** - Generates configurable number of RAG queries
- âœ… **Background Knowledge Acquisition** - Retrieves relevant knowledge using RAG
- âœ… **Question Planning** - Creates comprehensive plan with question IDs and focuses
- âœ… **Single-Pass Relevance Analysis** - Analyzes question relevance (no iteration/rejection)
- âœ… **Complete File Persistence** - Saves knowledge, plan, and individual results

### Mimic Mode (Reference Exam-Based)
- âœ… **PDF Parsing** - Automatic PDF extraction using MinerU
- âœ… **Question Extraction** - Identifies and extracts reference questions from exams
- âœ… **Style Mimicking** - Generates similar questions based on reference structure
- âœ… **Batch Organization** - All outputs saved to timestamped folders

### Common Features
- âœ… **Unified BaseAgent** - All agents inherit from `src/agents/base_agent.py`
- âœ… **No Iteration Loops** - Single-pass generation + analysis
- âœ… **Bilingual Prompts** - Supports both Chinese and English via YAML
- âœ… **Real-time WebSocket Streaming** - Live progress updates to frontend

## System Architecture

### Modular Agent Architecture

```
User Requirement
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AgentCoordinator                                â”‚
â”‚                                                 â”‚
â”‚  1. RetrieveAgent                               â”‚
â”‚     â”œâ”€â”€ Generate RAG queries (LLM)              â”‚
â”‚     â””â”€â”€ Execute RAG searches (parallel)         â”‚
â”‚                                                 â”‚
â”‚  2. Plan Generation (in coordinator)            â”‚
â”‚     â””â”€â”€ Create focuses for each question        â”‚
â”‚                                                 â”‚
â”‚  3. GenerateAgent (per question)                â”‚
â”‚     â””â”€â”€ Generate question from knowledge+focus  â”‚
â”‚                                                 â”‚
â”‚  4. RelevanceAnalyzer (per question)            â”‚
â”‚     â””â”€â”€ Analyze KB relevance (high/partial)     â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Save Results:
  - knowledge.json (queries + retrievals)
  - plan.json (focuses)
  - q_1/result.json, q_1/question.md
  - q_2/result.json, q_2/question.md
  - summary.json
```

### Mimic Mode Architecture

```
PDF Upload / Parsed Directory
    â†“
Parse PDF with MinerU (tools/pdf_parser.py)
    â†“
Extract Reference Questions (tools/question_extractor.py)
    â†“
For each reference question (parallel):
    AgentCoordinator.generate_question(with_reference=True)
    â”œâ”€â”€ RetrieveAgent
    â”œâ”€â”€ GenerateAgent (with reference)
    â””â”€â”€ RelevanceAnalyzer
    â†“
Save to timestamped folder
```

## Core Components

### 1. RetrieveAgent

Located in `agents/retrieve_agent.py`. Inherits from `src/agents/base_agent.py`.

**Responsibilities:**
- Generate semantic search queries from requirements
- Execute RAG searches in parallel
- Merge and summarize retrieval results

```python
retrieve_agent = RetrieveAgent(kb_name="math2211", language="en")
result = await retrieve_agent.process(requirement, num_queries=3)
# Returns: {"queries": [...], "retrievals": [...], "summary": "...", "has_content": True}
```

### 2. GenerateAgent

Located in `agents/generate_agent.py`. Generates questions based on knowledge context.

**Responsibilities:**
- Generate custom questions from requirements + knowledge
- Generate mimic questions from reference + knowledge
- Output structured question JSON

```python
generate_agent = GenerateAgent(language="en")
result = await generate_agent.process(
    requirement={"knowledge_point": "...", "difficulty": "medium"},
    knowledge_context="...",
    focus={"focus": "...", "type": "written"},
    reference_question=None  # or "reference text" for mimic mode
)
# Returns: {"success": True, "question": {...}}
```

### 3. RelevanceAnalyzer

Located in `agents/relevance_analyzer.py`. Replaces old validation workflow.

**Key difference from old ValidationWorkflow:**
- **NO rejection**: all questions are accepted
- **NO iteration**: single-pass analysis
- Output: relevance level (high/partial) with explanations

```python
analyzer = RelevanceAnalyzer(language="en")
result = await analyzer.process(question={"question": "..."}, knowledge_context="...")
# Returns: {"relevance": "high", "kb_coverage": "...", "extension_points": ""}
```

### 4. AgentCoordinator

Located in `coordinator.py`. Orchestrates the workflow using specialized agents.

**Methods:**
- `generate_question()`: Single question generation (used by Mimic mode)
- `generate_questions_custom()`: Batch generation from requirement

## Usage

### Custom Mode - Batch Generation

```python
import asyncio
from src.agents.question import AgentCoordinator

async def main():
    coordinator = AgentCoordinator(
        kb_name="math2211",
        output_dir="data/user/question",
        language="en"
    )

    result = await coordinator.generate_questions_custom(
        requirement={
            "knowledge_point": "Multivariable limits",
            "difficulty": "medium",
            "question_type": "choice"
        },
        num_questions=3
    )

    print(f"âœ… Generated {result['completed']}/{result['requested']} questions")
    for r in result['results']:
        print(f"- [{r['analysis']['relevance']}] {r['question']['question'][:50]}...")

asyncio.run(main())
```

### Custom Mode - Single Question

```python
requirement = {
    "knowledge_point": "Limits and continuity",
    "difficulty": "medium",
    "question_type": "choice"
}

result = await coordinator.generate_question(requirement)

if result["success"]:
    print(f"Question: {result['question']['question']}")
    print(f"Relevance: {result['validation']['relevance']}")
    print(f"KB Coverage: {result['validation']['kb_coverage']}")
```

### Mimic Mode - PDF Upload

```python
from src.tools.question import mimic_exam_questions

result = await mimic_exam_questions(
    pdf_path="exams/midterm.pdf",
    kb_name="math2211",
    output_dir="data/user/question/mimic_papers",
    max_questions=5
)

print(f"âœ… Generated {result['successful_generations']} questions")
```

### Mimic Mode - Parsed Directory

```python
result = await mimic_exam_questions(
    paper_dir="data/parsed_exams/exam_20240101",
    kb_name="math2211"
)
```

## Configuration

### `config/main.yaml`

```yaml
question:
  # Refactored: no iteration loops (max_rounds removed)
  rag_query_count: 3
  max_parallel_questions: 1
  rag_mode: naive
  agents:
    retrieve:
      top_k: 30
    generate:
      max_retries: 2
    relevance_analyzer:
      enabled: true
```

### `config/agents.yaml`

```yaml
question:
  temperature: 0.7
  max_tokens: 4000
```

## Return Format

### Single Question Result

```python
{
    "success": True,
    "question": {
        "question_type": "choice",
        "question": "Question content",
        "options": {"A": "...", "B": "...", "C": "...", "D": "..."},
        "correct_answer": "A",
        "explanation": "Detailed explanation",
        "knowledge_point": "Topic name"
    },
    "validation": {
        "decision": "approve",  # Always approve
        "relevance": "high",    # or "partial"
        "kb_coverage": "This question tests...",
        "extension_points": ""  # Only if relevance is "partial"
    },
    "rounds": 1  # Always 1 (no iteration)
}
```

### Batch Generation Result

```python
{
    "success": True,
    "requested": 3,
    "completed": 3,
    "failed": 0,
    "search_queries": ["query1", "query2", "query3"],
    "plan": {"focuses": [...]},
    "results": [
        {
            "question_id": "q_1",
            "focus": {"focus": "...", "type": "written"},
            "question": {...},
            "analysis": {"relevance": "high", "kb_coverage": "..."}
        },
        ...
    ],
    "failures": []
}
```

## Output Files

### Custom Mode Directory Structure

```
data/user/question/batch_YYYYMMDD_HHMMSS/
â”œâ”€â”€ knowledge.json       # RAG queries and retrievals
â”œâ”€â”€ plan.json            # Question focuses
â”œâ”€â”€ q_1/
â”‚   â”œâ”€â”€ result.json      # Question + analysis
â”‚   â””â”€â”€ question.md      # Human-readable format
â”œâ”€â”€ q_2/
â”‚   â”œâ”€â”€ result.json
â”‚   â””â”€â”€ question.md
â””â”€â”€ summary.json         # Overall summary
```

### Mimic Mode Directory Structure

```
data/user/question/mimic_papers/{paper_name}/
â”œâ”€â”€ auto/{paper_name}.md                              # MinerU parsed markdown
â”œâ”€â”€ {paper_name}_YYYYMMDD_HHMMSS_questions.json       # Extracted reference questions
â””â”€â”€ {paper_name}_YYYYMMDD_HHMMSS_generated.json       # Generated questions
```

## Key Changes from Previous Version

### âœ… Architecture Changes

1. **Unified BaseAgent**
   - All agents now inherit from `src/agents/base_agent.py`
   - Automatic LLM config, prompt loading, token tracking
   - No more separate ReAct base class

2. **Specialized Agents**
   - `RetrieveAgent`: Knowledge retrieval only
   - `GenerateAgent`: Question generation only
   - `RelevanceAnalyzer`: Relevance analysis only (replaces ValidationWorkflow)

3. **No Iteration/Rejection**
   - Removed `max_rounds` from config
   - Single-pass generation + analysis
   - All questions accepted with relevance classification

4. **Tools Moved**
   - `src/agents/question/tools/` â†’ `src/tools/question/`
   - Import via `from src.tools.question import ...`

### âŒ Removed

1. **Old ReAct Architecture**
   - `agents/base_agent.py` (ReAct paradigm)
   - `agents/generation_agent.py` (ReAct-based)
   - `agents/validation_agent.py` (deprecated)
   - `validation_workflow.py` (replaced by RelevanceAnalyzer)

2. **Iteration Logic**
   - No more validation loops
   - No more task rejection
   - No more request_modification/request_regeneration

3. **Message Queue**
   - Agent communication simplified
   - Direct method calls instead of message passing

## Migration Notes

**Import Path Updates:**

```python
# Old
from src.agents.question import BaseAgent, QuestionGenerationAgent
from src.tools.question import mimic_exam_questions

# New
from src.agents.question import RetrieveAgent, GenerateAgent, RelevanceAnalyzer
from src.tools.question import mimic_exam_questions
```

**API Changes:**

```python
# Old - with rejection handling
result = await coordinator.generate_question(requirement)
if result.get("error") == "task_rejected":
    print("Agent rejected task")

# New - always generates, check relevance
result = await coordinator.generate_question(requirement)
if result["success"]:
    if result["validation"]["relevance"] == "high":
        print("Fully covered by KB:", result["validation"]["kb_coverage"])
    else:
        print("Extension question:", result["validation"]["extension_points"])
```
